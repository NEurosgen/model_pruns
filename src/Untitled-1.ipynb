{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20ff1d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eager PTQ:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/temp/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: torch.Size([1, 10])\n",
      "\n",
      "FX PTQ:\n",
      "OK: torch.Size([1, 10])\n",
      "\n",
      "PT2E PTQ:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------- Toy CNN ----------\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 16, 3, stride=2, padding=1)  # 112x112\n",
    "        self.bn   = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc   = nn.Linear(16 * 112 * 112, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn(self.conv(x)))\n",
    "        x = x.flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def calib_loader(n_batches=32, bs=4, shape=(3, 224, 224), device=\"cpu\"):\n",
    "    for _ in range(n_batches):\n",
    "        yield torch.randn(bs, *shape, device=device), None\n",
    "\n",
    "\n",
    "def fuse_smallcnn(m: nn.Module):\n",
    "    m.eval()\n",
    "    torch.ao.quantization.fuse_modules(m, [[\"conv\", \"bn\", \"relu\"]], inplace=True)\n",
    "    return m\n",
    "\n",
    "\n",
    "# -------------------- A) Eager PTQ --------------------\n",
    "def eager_ptq(model: nn.Module):\n",
    "    from torch.ao.quantization import (\n",
    "        QuantStub, DeQuantStub, get_default_qconfig, prepare, convert\n",
    "    )\n",
    "    torch.backends.quantized.engine = \"fbgemm\"\n",
    "\n",
    "    class QuantWrapper(nn.Module):\n",
    "        def __init__(self, m):\n",
    "            super().__init__()\n",
    "            self.q = QuantStub(); self.m = m; self.dq = DeQuantStub()\n",
    "        def forward(self, x): return self.dq(self.m(self.q(x)))\n",
    "\n",
    "    model = model.to(\"cpu\").eval()\n",
    "    fuse_smallcnn(model)\n",
    "    qmodel = QuantWrapper(model)\n",
    "    qmodel.qconfig = get_default_qconfig(torch.backends.quantized.engine)\n",
    "\n",
    "    prepare(qmodel, inplace=True)\n",
    "    with torch.no_grad():\n",
    "        for x, _ in calib_loader(n_batches=32):\n",
    "            qmodel(x)\n",
    "\n",
    "    convert(qmodel, inplace=True)\n",
    "    return qmodel\n",
    "\n",
    "\n",
    "# -------------------- B) FX PTQ --------------------\n",
    "def fx_ptq(model: nn.Module):\n",
    "    from torch.ao.quantization import get_default_qconfig\n",
    "    from torch.ao.quantization.quantize_fx import QConfigMapping, prepare_fx, convert_fx\n",
    "\n",
    "    torch.backends.quantized.engine = \"fbgemm\"\n",
    "    model = model.to(\"cpu\").eval()\n",
    "\n",
    "    if isinstance(getattr(model, \"conv\", None), nn.Conv2d) and \\\n",
    "       isinstance(getattr(model, \"bn\",   None), nn.BatchNorm2d) and \\\n",
    "       isinstance(getattr(model, \"relu\", None), nn.ReLU):\n",
    "        torch.ao.quantization.fuse_modules(model, [[\"conv\", \"bn\", \"relu\"]], inplace=True)\n",
    "\n",
    "    qconfig = get_default_qconfig(torch.backends.quantized.engine)\n",
    "    qmap = QConfigMapping().set_global(qconfig)\n",
    "\n",
    "    example_inputs = (torch.randn(1, 3, 224, 224),)\n",
    "    prepared = prepare_fx(model, qmap, example_inputs=example_inputs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in calib_loader(n_batches=32):\n",
    "            prepared(x)\n",
    "\n",
    "    quantized = convert_fx(prepared)\n",
    "    return quantized\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- run demo ----------\n",
    "if __name__ == \"__main__\":\n",
    "    import copy\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    base = SmallCNN()\n",
    "\n",
    "    print(\"\\nEager PTQ:\")\n",
    "    qm_eager = eager_ptq(copy.deepcopy(base))\n",
    "    print(\"OK:\", qm_eager(torch.randn(1,3,224,224)).shape)\n",
    "\n",
    "    print(\"\\nFX PTQ:\")\n",
    "    qm_fx = fx_ptq(copy.deepcopy(base))\n",
    "    print(\"OK:\", qm_fx(torch.randn(1,3,224,224)).shape)\n",
    "\n",
    "    print(\"\\nPT2E PTQ:\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ebe789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchao.quantization.pt2e.quantize_pt2e import prepare_pt2e, convert_pt2e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b7f03da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32   :   10.01 ms/iter   |     798.9 imgs/s\n",
      "INT8 PTQ:    8.35 ms/iter   |     957.9 imgs/s\n",
      "Output shapes: torch.Size([1, 10]) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Make timings more stable\n",
    "torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "torch.backends.quantized.engine = \"fbgemm\"  # use \"qnnpack\" on ARM\n",
    "\n",
    "# ---------------- Model ----------------\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 16, 3, stride=2, padding=1)  # 112x112 for 224x224 input\n",
    "        self.bn   = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc   = nn.Linear(16 * 112 * 112, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn(self.conv(x)))\n",
    "        x = x.flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def fuse_smallcnn(m: nn.Module):\n",
    "    m.eval()\n",
    "    # fuse conv-bn-relu -> conv_relu (quant-friendly)\n",
    "    torch.ao.quantization.fuse_modules(m, [[\"conv\", \"bn\", \"relu\"]], inplace=True)\n",
    "    return m\n",
    "\n",
    "# ------------- Calibration data -------------\n",
    "def calib_loader(n_batches=32, bs=4, shape=(3, 224, 224), device=\"cpu\"):\n",
    "    for _ in range(n_batches):\n",
    "        yield torch.randn(bs, *shape, device=device), None\n",
    "\n",
    "# ------------- FX static PTQ -------------\n",
    "def fx_ptq(model: nn.Module):\n",
    "    from torch.ao.quantization import get_default_qconfig\n",
    "    from torch.ao.quantization.quantize_fx import QConfigMapping, prepare_fx, convert_fx\n",
    "\n",
    "    model = model.to(\"cpu\").eval()\n",
    "    # optional fusion (important for best int8 patterns)\n",
    "    if isinstance(getattr(model, \"conv\", None), nn.Conv2d) and \\\n",
    "       isinstance(getattr(model, \"bn\",   None), nn.BatchNorm2d) and \\\n",
    "       isinstance(getattr(model, \"relu\", None), nn.ReLU):\n",
    "        fuse_smallcnn(model)\n",
    "\n",
    "    qconfig = get_default_qconfig(torch.backends.quantized.engine)\n",
    "    qmap = QConfigMapping().set_global(qconfig)\n",
    "\n",
    "    example_inputs = (torch.randn(1, 3, 224, 224),)\n",
    "    prepared = prepare_fx(model, qmap, example_inputs=example_inputs)\n",
    "\n",
    "    # Calibrate observers\n",
    "    with torch.no_grad():\n",
    "        for x, _ in calib_loader(n_batches=32):\n",
    "            prepared(x)\n",
    "\n",
    "    quantized = convert_fx(prepared)\n",
    "    return quantized\n",
    "\n",
    "# ------------- Benchmark -------------\n",
    "@torch.inference_mode()\n",
    "def benchmark(model: nn.Module, bs=8, iters=100, warmup=10, device=\"cpu\"):\n",
    "    model = model.to(device).eval()\n",
    "    x = torch.randn(bs, 3, 224, 224, device=device)\n",
    "    # warmup\n",
    "    for _ in range(warmup):\n",
    "        model(x)\n",
    "    # measure\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        model(x)\n",
    "    dt = time.perf_counter() - t0\n",
    "    latency_ms = (dt / iters) * 1000.0\n",
    "    throughput = (bs * iters) / dt\n",
    "    return latency_ms, throughput\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    base = SmallCNN().eval().to(\"cpu\")\n",
    "\n",
    "    # FP32\n",
    "    fp32_lat, fp32_ips = benchmark(base, bs=8, iters=100, warmup=10)\n",
    "    print(f\"FP32   : {fp32_lat:7.2f} ms/iter   |  {fp32_ips:8.1f} imgs/s\")\n",
    "\n",
    "    # INT8 (FX PTQ)\n",
    "    int8_model = fx_ptq(SmallCNN().eval())  # fresh copy for fair compare\n",
    "    int8_lat, int8_ips = benchmark(int8_model, bs=8, iters=100, warmup=10)\n",
    "    print(f\"INT8 PTQ: {int8_lat:7.2f} ms/iter   |  {int8_ips:8.1f} imgs/s\")\n",
    "\n",
    "    # Quick sanity: same output shape\n",
    "    out_fp32 = base(torch.randn(1,3,224,224))\n",
    "    out_int8 = int8_model(torch.randn(1,3,224,224))\n",
    "    print(\"Output shapes:\", out_fp32.shape, out_int8.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc9192ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Drons'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcheckpoints_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint_load\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model_from_checkpoint\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m  \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpruning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_prunning\u001b[39;00m\u001b[38;5;250m  \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m global_unstructured_prune,make_pruning_permanent\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreport_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeasure_latency\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m measure_latency\n",
      "File \u001b[0;32m~/MyDir/Projects/aspdfpwjfpwejfwpefwef/Optimization/src/checkpoints_utils/checkpoint_load.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01momegaconf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DictConfig, OmegaConf\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mDrons\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcreate_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_model  \n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_load_checkpoint\u001b[39m(path: Path \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m, map_location: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a PyTorch Lightning checkpoint.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Drons'"
     ]
    }
   ],
   "source": [
    "from checkpoints_utils.checkpoint_load import load_model_from_checkpoint\n",
    "from  .pruning.tensor_prunning  import global_unstructured_prune,make_pruning_permanent\n",
    "from .report_utils.measure_latency import measure_latency\n",
    "from .report_utils.report_sparsity import report_sparsity\n",
    "from .pruning.channel_pruning import prune_model_channels\n",
    "from .pruning.tensor_prunning import threshold_prune\n",
    "from  omegaconf import OmegaConf\n",
    "from .pruning.channel_pruning import progressive_channel_pruning\n",
    "cfg = OmegaConf.load(\"/home/temp/MyDir/Projects/aspdfpwjfpwejfwpefwef/Drons/tb_logs_big/efficientnet/version_6/hparams.yaml\")\n",
    "\n",
    "\n",
    "model = load_model_from_checkpoint(checkpoint_path=\"tb_logs_big/efficientnet/version_0/checkpoints/epoch=1-step=88.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9e9141",
   "metadata": {},
   "source": [
    "Значит eficientnet с квантимщайцией по скорости как mobilnet с  эмитауией квантизщацйией если пеолциятся выбить тоже качастов на moblinet что и на efiicrentnet ,то победв"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
